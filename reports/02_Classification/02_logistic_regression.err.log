Traceback (most recent call last):
  File "/Users/eatai/.pyenv/versions/3.13.1/envs/datascience/lib/python3.13/site-packages/jupyter_cache/executors/utils.py", line 58, in single_nb_execution
    executenb(
    ~~~~~~~~~^
        nb,
        ^^^
    ...<4 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "/Users/eatai/.pyenv/versions/3.13.1/envs/datascience/lib/python3.13/site-packages/nbclient/client.py", line 1319, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "/Users/eatai/.pyenv/versions/3.13.1/envs/datascience/lib/python3.13/site-packages/jupyter_core/utils/__init__.py", line 165, in wrapped
    return loop.run_until_complete(inner)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^
  File "/Users/eatai/.pyenv/versions/3.13.1/lib/python3.13/asyncio/base_events.py", line 720, in run_until_complete
    return future.result()
           ~~~~~~~~~~~~~^^
  File "/Users/eatai/.pyenv/versions/3.13.1/envs/datascience/lib/python3.13/site-packages/nbclient/client.py", line 709, in async_execute
    await self.async_execute_cell(
        cell, index, execution_count=self.code_cells_executed + 1
    )
  File "/Users/eatai/.pyenv/versions/3.13.1/envs/datascience/lib/python3.13/site-packages/nbclient/client.py", line 1062, in async_execute_cell
    await self._check_raise_for_error(cell, cell_index, exec_reply)
  File "/Users/eatai/.pyenv/versions/3.13.1/envs/datascience/lib/python3.13/site-packages/nbclient/client.py", line 918, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
from sklearn.preprocessing import StandardScaler

X = bc_df.drop('y', axis = 1)

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the data, since we are using multiple features
ss = StandardScaler()
X_train_scaled = ss.fit_transform(X_train)
X_test_scaled = ss.fit_transform(X_test)

# Train a logistic regression model
model_LogReg = LogisticRegression(penalty = None, max_iter = 10000)
model_LogReg.fit(X_train_scaled, y_train)

# Make predictions
y_pred_train = model_LogReg.predict(X_train_scaled)
y_pred = model_LogReg.predict(X_test_scaled)

y_pred_prob = model_LogReg.predict_proba(X_test_scaled)


------------------


[0;31m---------------------------------------------------------------------------[0m
[0;31mValueError[0m                                Traceback (most recent call last)
Cell [0;32mIn[14], line 6[0m
[1;32m      3[0m X [38;5;241m=[39m bc_df[38;5;241m.[39mdrop([38;5;124m'[39m[38;5;124my[39m[38;5;124m'[39m, axis [38;5;241m=[39m [38;5;241m1[39m)
[1;32m      5[0m [38;5;66;03m# Split the dataset into training and testing sets[39;00m
[0;32m----> 6[0m X_train, X_test, y_train, y_test [38;5;241m=[39m [43mtrain_test_split[49m[43m([49m[43mX[49m[43m,[49m[43m [49m[43my[49m[43m,[49m[43m [49m[43mtest_size[49m[38;5;241;43m=[39;49m[38;5;241;43m0.2[39;49m[43m,[49m[43m [49m[43mrandom_state[49m[38;5;241;43m=[39;49m[38;5;241;43m42[39;49m[43m)[49m
[1;32m      8[0m [38;5;66;03m# Scale the data, since we are using multiple features[39;00m
[1;32m      9[0m ss [38;5;241m=[39m StandardScaler()

File [0;32m~/.pyenv/versions/3.13.1/envs/datascience/lib/python3.13/site-packages/sklearn/utils/_param_validation.py:216[0m, in [0;36mvalidate_params.<locals>.decorator.<locals>.wrapper[0;34m(*args, **kwargs)[0m
[1;32m    210[0m [38;5;28;01mtry[39;00m:
[1;32m    211[0m     [38;5;28;01mwith[39;00m config_context(
[1;32m    212[0m         skip_parameter_validation[38;5;241m=[39m(
[1;32m    213[0m             prefer_skip_nested_validation [38;5;129;01mor[39;00m global_skip_validation
[1;32m    214[0m         )
[1;32m    215[0m     ):
[0;32m--> 216[0m         [38;5;28;01mreturn[39;00m [43mfunc[49m[43m([49m[38;5;241;43m*[39;49m[43margs[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mkwargs[49m[43m)[49m
[1;32m    217[0m [38;5;28;01mexcept[39;00m InvalidParameterError [38;5;28;01mas[39;00m e:
[1;32m    218[0m     [38;5;66;03m# When the function is just a wrapper around an estimator, we allow[39;00m
[1;32m    219[0m     [38;5;66;03m# the function to delegate validation to the estimator, but we replace[39;00m
[1;32m    220[0m     [38;5;66;03m# the name of the estimator by the name of the function in the error[39;00m
[1;32m    221[0m     [38;5;66;03m# message to avoid confusion.[39;00m
[1;32m    222[0m     msg [38;5;241m=[39m re[38;5;241m.[39msub(
[1;32m    223[0m         [38;5;124mr[39m[38;5;124m"[39m[38;5;124mparameter of [39m[38;5;124m\[39m[38;5;124mw+ must be[39m[38;5;124m"[39m,
[1;32m    224[0m         [38;5;124mf[39m[38;5;124m"[39m[38;5;124mparameter of [39m[38;5;132;01m{[39;00mfunc[38;5;241m.[39m[38;5;18m__qualname__[39m[38;5;132;01m}[39;00m[38;5;124m must be[39m[38;5;124m"[39m,
[1;32m    225[0m         [38;5;28mstr[39m(e),
[1;32m    226[0m     )

File [0;32m~/.pyenv/versions/3.13.1/envs/datascience/lib/python3.13/site-packages/sklearn/model_selection/_split.py:2848[0m, in [0;36mtrain_test_split[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)[0m
[1;32m   2845[0m [38;5;28;01mif[39;00m n_arrays [38;5;241m==[39m [38;5;241m0[39m:
[1;32m   2846[0m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m([38;5;124m"[39m[38;5;124mAt least one array required as input[39m[38;5;124m"[39m)
[0;32m-> 2848[0m arrays [38;5;241m=[39m [43mindexable[49m[43m([49m[38;5;241;43m*[39;49m[43marrays[49m[43m)[49m
[1;32m   2850[0m n_samples [38;5;241m=[39m _num_samples(arrays[[38;5;241m0[39m])
[1;32m   2851[0m n_train, n_test [38;5;241m=[39m _validate_shuffle_split(
[1;32m   2852[0m     n_samples, test_size, train_size, default_test_size[38;5;241m=[39m[38;5;241m0.25[39m
[1;32m   2853[0m )

File [0;32m~/.pyenv/versions/3.13.1/envs/datascience/lib/python3.13/site-packages/sklearn/utils/validation.py:532[0m, in [0;36mindexable[0;34m(*iterables)[0m
[1;32m    502[0m [38;5;250m[39m[38;5;124;03m"""Make arrays indexable for cross-validation.[39;00m
[1;32m    503[0m 
[1;32m    504[0m [38;5;124;03mChecks consistent length, passes through None, and ensures that everything[39;00m
[0;32m   (...)[0m
[1;32m    528[0m [38;5;124;03m[[1, 2, 3], array([2, 3, 4]), None, <...Sparse...dtype 'int64'...shape (3, 1)>][39;00m
[1;32m    529[0m [38;5;124;03m"""[39;00m
[1;32m    531[0m result [38;5;241m=[39m [_make_indexable(X) [38;5;28;01mfor[39;00m X [38;5;129;01min[39;00m iterables]
[0;32m--> 532[0m [43mcheck_consistent_length[49m[43m([49m[38;5;241;43m*[39;49m[43mresult[49m[43m)[49m
[1;32m    533[0m [38;5;28;01mreturn[39;00m result

File [0;32m~/.pyenv/versions/3.13.1/envs/datascience/lib/python3.13/site-packages/sklearn/utils/validation.py:475[0m, in [0;36mcheck_consistent_length[0;34m(*arrays)[0m
[1;32m    473[0m uniques [38;5;241m=[39m np[38;5;241m.[39munique(lengths)
[1;32m    474[0m [38;5;28;01mif[39;00m [38;5;28mlen[39m(uniques) [38;5;241m>[39m [38;5;241m1[39m:
[0;32m--> 475[0m     [38;5;28;01mraise[39;00m [38;5;167;01mValueError[39;00m(
[1;32m    476[0m         [38;5;124m"[39m[38;5;124mFound input variables with inconsistent numbers of samples: [39m[38;5;132;01m%r[39;00m[38;5;124m"[39m
[1;32m    477[0m         [38;5;241m%[39m [[38;5;28mint[39m(l) [38;5;28;01mfor[39;00m l [38;5;129;01min[39;00m lengths]
[1;32m    478[0m     )

[0;31mValueError[0m: Found input variables with inconsistent numbers of samples: [569, 1100]

